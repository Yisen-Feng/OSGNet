dataset_name: ego4d
devices: cuda:0
train_split: ['training']
val_split: ['validation']
model_name: ObjectLocPointTransformer
dataset: {
  json_file: ./ego4d_data/ego4d_nlq_v2_ori_data/nlq_val.json,
  train_jsonl_file: /home/feng_yi_sen/GroundNLQ-DINO/ego4d_data/narrations/format_unique_pretrain_data_v2.jsonl,
  val_jsonl_file: ./ego4d_data/ego4d_nlq_val_v2_label_lemma.jsonl,
  video_feat_dir: /root/autodl-tmp/data/ego4d/nlq/em_egovlp+internvideo_visual_features_1.87fps,
  text_feat_dir: /root/autodl-tmp/data/ego4d/nlq/narration/narration_clip_token_features,
  val_text_feat_dir: /root/autodl-tmp/data/ego4d/nlq/nlq_v2_clip_token_features,
  num_classes: 1,
  input_vid_dim: 2304,
  input_txt_dim: 512,
  feat_stride: 16.0,
  num_frames: 16.0,
  default_fps: 30,
  max_seq_len: 2560,
}
model: {
  backbone_type: ObjectMambaTransformer,
  fpn_type: identity,
  max_buffer_len_factor: 4.0,
  n_mha_win_size: 9,
  backbone_arch: [2, 4, 4, 0, 6],
  # shrink the model for reduced input feature channels
  object_dim: 512,
  object_win_size: 1,
  object_use_cross_model: True,
  n_head: 4,
  embd_dim: 512,
  fpn_dim: 512,
  head_dim: 512,
  use_abs_pe: True,
  regression_range: [[0, 4], [2, 8], [4, 16], [8, 32], [16, 64], [32, 128],[64, 10000]],
  
}
opt: {
  learning_rate: 0.0002,
  backbone_lr_weight: 1,
  epochs: 6,
  warmup_epochs: 4,
  weight_decay: 0.05,
}
loader: {
  batch_size: 4,
  num_workers: 4,
}
train_cfg: {
  init_loss_norm: 200,
  clip_grad_l2norm: 1.0,
  cls_prior_prob: 0.01,
  center_sample: radius,
  center_sample_radius: 1.5,
  label_smoothing: 0.1,
  droppath: 0.1,
  loss_weight: 1.0,
  mamba_arch: ['bimamba1','mlp','none']
}
test_cfg: {
  voting_thresh: 0.9,
  pre_nms_topk: 2000,
  # max of 50 predictions per video
  max_seg_num: 5,
  min_score: 0.001,
  nms_sigma : 0.75,
  duration_thresh: 0.001,
  test_num: 1,
  test_start_epoch: 2
}
output_folder: /root/autodl-tmp/model/GroundNLQ/ckpt/
